# Analystt.ai
Assignment of Web Scraping Amazon Website using BeautifulSoup, requests and pandas.

Language used: Python
Libraries used: BeautifulSoup,Pandas and Requests.

This is the assignment of Analystt.ai where I have scraped around 20 pages of amazon website.
I made an csv using pandas and stored product url, product name, product price, product reviews and ratings

Analystt.ai_assignment.ipynb: It consists of code which i wrote for scraping amazon pages.
amazon_products.csv: This is generated file using pandas.

Part1 + Part2.xlsx : This is the file where i stored all the final and cleaned data.

With the help of this Assignment I got to learn various form of writing same code in easy way, and i got to learn excel shortcuts.
This assignment helped me to explore more technologies for web scraping. 

Mian Purpose of Web Scraping :

1. Data Collection: Web scraping can be used to collect large amounts of data from websites, which can be used for research, analysis, or business intelligence purposes.

2. Price Comparison: Web scraping can be used to compare prices of products across different websites, helping individuals and organizations save money.

3. Lead Generation: Web scraping can be used to collect information about potential customers or clients, helping organizations generate leads and improve their marketing efforts.

4. Competitor Analysis: Web scraping can be used to collect information about competitor websites, helping organizations gain insights into their competitors and improve their own offerings.

5. Content Creation: Web scraping can be used to collect data from websites, which can be used as inspiration or reference material for content creation, such as writing articles or creating infographics.



Web scraping can provide valuable data and insights that can help individuals and organizations make informed decisions and improve their operations. However, it's important to follow ethical and legal guidelines when conducting web scraping projects, such as respecting website owners' copyrights and avoiding scraping sensitive or confidential information.
